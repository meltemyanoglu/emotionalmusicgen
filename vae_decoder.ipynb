{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d72aa392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from utils import _latentspace\n",
    "from utils._modeltraining import EmotionalVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9644579a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmotionalVAE model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Load the pre-trained EmotionalVAE model ---\n",
    "latent_dim = 750\n",
    "hidden_dims = [512, 256, 128]\n",
    "condition_dim = 2\n",
    "model = EmotionalVAE(latent_dim, hidden_dims, condition_dim)\n",
    "model_path = r'vae_model\\final_model.pt'\n",
    "checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "print(\"EmotionalVAE model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f5b19b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent tokens generated.\n"
     ]
    }
   ],
   "source": [
    "# --- Generate latent tokens using sample() ---\n",
    "# Define the number of samples and an example emotional condition (valence, arousal)\n",
    "num_samples = 5\n",
    "condition = torch.tensor([0.7, 0.6])  # Adjust values as necessary\n",
    "if len(condition.shape) == 1:\n",
    "    condition = condition.unsqueeze(0)\n",
    "condition = condition.repeat(num_samples, 1)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "latent_tokens = model.sample(num_samples, condition, device)\n",
    "# latent_tokens shape: [num_samples, quantizer_count, latent_dim]\n",
    "print(\"Latent tokens generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fff8ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading EnCodec model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Huawei\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnCodec model loaded successfully (target bandwidth: 6.0 kbps)\n"
     ]
    }
   ],
   "source": [
    "# --- Feed the latent tokens into EnCodec decoder ---\n",
    "# Initialize the LatentRepresentationGenerator to access the EnCodec model and its decoder.\n",
    "# Here we assume that the EnCodec model is loaded inside LatentRepresentationGenerator via _load_encodec_model().\n",
    "\n",
    "# Adjust parameters as needed\n",
    "lat_gen = _latentspace.LatentRepresentationGenerator(\n",
    "    encodec_bandwidth=6.0,\n",
    "    device=device,\n",
    "    chunk_sizes=[5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdb473b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading EnCodec model...\n",
      "EnCodec model loaded successfully (target bandwidth: 6.0 kbps)\n",
      "EnCodec decoder loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Ensure the EnCodec model is loaded\n",
    "lat_gen._load_encodec_model()\n",
    "print(\"EnCodec decoder loaded successfully.\")\n",
    "\n",
    "# Prepare the latent tokens for decoding.\n",
    "# The EnCodec decoder expects tokens in a specific dictionary format.\n",
    "# We assume each sample uses layer 0 tokens.\n",
    "decoded_audios = []\n",
    "for sample in latent_tokens:\n",
    "    # Convert sample to tensor of the right shape if needed.\n",
    "    # Instead of preparing a dictionary, get the tensor for layer 0 directly.\n",
    "    codes_tensor = sample.unsqueeze(0).long() \n",
    "    scale = torch.ones(1, 1).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        codes = sample.unsqueeze(0)              # float tensor\n",
    "        codes_int = codes.round().long()         # yuvarla → tamsayı\n",
    "        codes_int = codes_int.clamp(0, 255)      # örnek aralık\n",
    "        decoded_audio = lat_gen.encodec_model.decode([(codes_int, scale)])[0]\n",
    "    \n",
    "    # Move decoded audio to CPU and convert to NumPy array.\n",
    "    decoded_audio_np = decoded_audio.cpu().numpy()[0]  # Assuming mono channel\n",
    "    decoded_audios.append(decoded_audio_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "189672b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio sample saved: decoded_sample_0.wav\n",
      "Audio sample saved: decoded_sample_1.wav\n",
      "Audio sample saved: decoded_sample_2.wav\n",
      "Audio sample saved: decoded_sample_3.wav\n",
      "Audio sample saved: decoded_sample_4.wav\n",
      "Full audio synthesis pipeline complete: Audio → EnCodec Encoder → Custom Autoencoder → New tokens → EnCodec Decoder → Audio\n"
     ]
    }
   ],
   "source": [
    "# --- Save the decoded audio files ---\n",
    "sample_rate = lat_gen.encodec_model.sample_rate  # Use the EnCodec model's sample rate\n",
    "for i, audio in enumerate(decoded_audios):\n",
    "    output_path = f\"decoded_sample_{i}.wav\"\n",
    "    sf.write(output_path, audio, sample_rate)\n",
    "    print(f\"Audio sample saved: {output_path}\")\n",
    "\n",
    "# Final message\n",
    "print(\"Full audio synthesis pipeline complete: Audio → EnCodec Encoder → Custom Autoencoder → New tokens → EnCodec Decoder → Audio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321f0112",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
